This work presents a theoretical and empirical analysis of internal sorting algorithms, a fundamental operation in computer science with applications in several areas. The main objective is to compare the performance of 14 sorting algorithms, classified into inferior, superior, hybrid, and linear-time methods. The methodology consists of implementing these algorithms in the C programming language and conducting controlled experiments using vectors with up to $10^8$ elements under sorted, reverse-sorted, and pseudo-random distributions. The study collected metrics such as execution time, number of comparisons, data movements, and processor cache efficiency. The empirical results show that randomized Quicksort and Introsort achieve the best overall performance among comparison-based methods, while linear algorithms, such as Counting Sort and Radix Sort, outperform the others in specific scenarios, despite their higher memory consumption. The analysis highlights the importance of the memory hierarchy, demonstrating that algorithms with good locality of reference significantly optimize the sorting of large data volumes. The study concludes that selecting the most suitable algorithm requires a multidimensional analysis that considers hardware characteristics and the nature of the data distribution, since asymptotic analysis alone does not predict the impact of cache misses on real execution time.

\keywords{Sorting; Algorithms; Empirical analysis; Complexity; Cache efficiency.}
